{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDStandardScaler(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = StandardScaler(copy=True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "\n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(7165, 23, 520)\n",
      "(7165, 23)\n",
      "(7165,)\n",
      "[0.00000e+00 9.55000e-01 6.23990e+01 1.21611e+02 2.07960e+01 1.80670e+01\n",
      " 7.98560e+01 0.00000e+00 1.57062e+02 0.00000e+00 3.21346e+02 0.00000e+00\n",
      " 2.44688e+02 5.30000e-02 5.83330e+01 0.00000e+00 4.01000e-01 0.00000e+00\n",
      " 0.00000e+00 2.31000e-01 0.00000e+00 0.00000e+00 1.20000e-02 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 8.50000e-02 1.75900e+00\n",
      " 1.35310e+01 7.00950e+01 1.24182e+02]\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Get input feature dataset\n",
    "features = scipy.io.loadmat('/home/mehthab/feature_vector3.mat')\n",
    "\n",
    "\n",
    "\n",
    "# Turn feature dataset into seperate arrays\n",
    "AEVs = np.transpose(np.array(features['AEVs']), (2, 0, 1))\n",
    "Atomic_Num = np.array(features['Atomic_Num'], dtype=np.long)\n",
    "Target = np.array(features['labels'][0])\n",
    "'''\n",
    "\n",
    "AEVs = np.random.rand(7165, 23, 520)\n",
    "Atomic_Num = np.random.randint(6, size=(7165, 23))\n",
    "Target = np.random.rand(7165)\n",
    "\n",
    "'''\n",
    "AEVs = np.round(AEVs,4)\n",
    "#AEVs = normalize(AEVs)\n",
    "print(\"Shapes:\")\n",
    "print(np.shape(AEVs))\n",
    "print(np.shape(Atomic_Num))\n",
    "print(np.shape(Target))\n",
    "print(AEVs[50][2][41:80])\n",
    "list_atoms = []\n",
    "for row in Atomic_Num:\n",
    "    for elem in row:\n",
    "        list_atoms.append(elem)\n",
    "print(np.unique(np.array(list_atoms)))\n",
    "\n",
    "# Seperate into training and testing samples\n",
    "#train_atoms =    \n",
    "#train_samples = \n",
    "#train_labels = \n",
    "\n",
    "#test_atoms = \n",
    "#test_samples = \n",
    "#test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7165, 23, 520)\n"
     ]
    }
   ],
   "source": [
    "Ar= AEVs\n",
    "scaler = NDStandardScaler()\n",
    "data = scaler.fit_transform(Ar)\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.0634511   0.         -0.01765624 -0.05428271  0.\n",
      "  0.         -0.05807887  0.          0.          0.         -0.03446159\n",
      "  0.          0.          0.         -0.04986093  0.          0.\n",
      "  0.         -0.01619072 -0.01656901 -0.01769251 -0.02025765 -0.01974686\n",
      "  0.         -0.01925593  0.         -0.02027942  0.         -0.02029793\n",
      " -0.01639123 -0.01989542  0.         -0.01869644  0.          0.\n",
      " -0.02018886  0.          0.         -0.01181469]\n"
     ]
    }
   ],
   "source": [
    "print(data[50][3][280:320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(520, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = (self.fc4(x))\n",
    "        #print(x)\n",
    "        return x #need activation function on x or loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "epoch:  11\n",
      "epoch:  12\n",
      "epoch:  13\n",
      "epoch:  14\n",
      "epoch:  15\n",
      "epoch:  16\n",
      "epoch:  17\n",
      "epoch:  18\n",
      "epoch:  19\n",
      "molecule\n",
      "Target:  tensor([[-1314.5100]])\n",
      "Our_vl:  tensor([[-1311.3560]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.6541, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1471.8800]])\n",
      "Our_vl:  tensor([[-1469.9740]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.4060, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1224.2200]])\n",
      "Our_vl:  tensor([[-1152.7380]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(70.9819, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1365.7700]])\n",
      "Our_vl:  tensor([[-1311.3558]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(53.9142, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1767.1100]])\n",
      "Our_vl:  tensor([[-1787.2096]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(19.5996, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1480.5800]])\n",
      "Our_vl:  tensor([[-1437.5308]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(42.5492, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1760.2500]])\n",
      "Our_vl:  tensor([[-1787.2096]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(26.4596, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1191.4301]])\n",
      "Our_vl:  tensor([[-1185.1812]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(5.7489, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1762.0300]])\n",
      "Our_vl:  tensor([[-1787.2096]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(24.6796, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1754.9000]])\n",
      "Our_vl:  tensor([[-1787.2096]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(31.8096, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1636.4600]])\n",
      "Our_vl:  tensor([[-1661.0349]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(24.0750, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1509.6899]])\n",
      "Our_vl:  tensor([[-1502.4169]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(6.7731, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1466.3101]])\n",
      "Our_vl:  tensor([[-1469.9738]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3.1637, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1640.4000]])\n",
      "Our_vl:  tensor([[-1661.0348]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(20.1348, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1472.1500]])\n",
      "Our_vl:  tensor([[-1502.4169]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(29.7668, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1485.7200]])\n",
      "Our_vl:  tensor([[-1437.5308]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(47.6892, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1609.7300]])\n",
      "Our_vl:  tensor([[-1628.5916]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(18.3616, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1616.1899]])\n",
      "Our_vl:  tensor([[-1628.5916]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(11.9016, grad_fn=<SmoothL1LossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1362.8500]])\n",
      "Our_vl:  tensor([[-1343.7990]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(18.5510, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def doNN():\n",
    "    # Declare Nets\n",
    "    NNP_H = Net()\n",
    "    NNP_C = Net()\n",
    "    NNP_N = Net()\n",
    "    NNP_O = Net()\n",
    "    NNP_S = Net()\n",
    "    \n",
    "    #torch.nn.init.normal_(NNP_H.weight, mean=0, std=1)\n",
    "    #torch.nn.init.normal_(NNP_C.weight, mean=0, std=1)\n",
    "    #torch.nn.init.normal_(NNP_N.weight, mean=0, std=1)\n",
    "    #torch.nn.init.normal_(NNP_O.weight, mean=0, std=1)\n",
    "    #torch.nn.init.normal_(NNP_S.weight, mean=0, std=1)\n",
    "    \n",
    "    # put in a list for ease of access\n",
    "    nets = [NNP_H, NNP_C, NNP_N, NNP_O, NNP_S]\n",
    "\n",
    "        \n",
    "    # corresponding optimizers\n",
    "    optimizers = []\n",
    "    #criterions = []\n",
    "    for net in nets:\n",
    "        optimizers.append( optim.Adam(net.parameters(), lr=0.001,betas=(0.9,0.999) ,eps=1e-08))\n",
    "        #criterions.append(nn.MSELoss())\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    # wonder if we need seperate criterions\n",
    "    #criterion = nn.MSELoss()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    # training\n",
    "    epochs = 20\n",
    "    molecules = 300\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        print(\"epoch: \", epoch)\n",
    "        outp = []\n",
    "            \n",
    "        for molecule in range(molecules):\n",
    "        \n",
    "            # initialize optimizer\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "            out_f = torch.zeros(1,1)\n",
    "            \n",
    "            # use all relevant aev's on the relevant nets\n",
    "            for atom in range(23):\n",
    "                #print(Atomic_Num[molecule][atom], end=\" \"),\n",
    "                if(Atomic_Num[molecule][atom]==0):\n",
    "                    continue\n",
    "                \n",
    "                aev = torch.from_numpy(AEVs[molecule][atom])\n",
    "                out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "                #print(\"out\", out)\n",
    "                out_f = out_f + out\n",
    "                \n",
    "                '''\n",
    "                #extra\n",
    "                targett = Target[molecule]\n",
    "                targett = torch.from_numpy(np.array(targett)\n",
    "                targett = targett.view(1,-1)\n",
    "                loss = criterion(out, targett.float())\n",
    "                loss.backward()\n",
    "                optimizers[Atomic_Num[molecule][atom]-1].step()\n",
    "                '''\n",
    "            \n",
    "        \n",
    "            ''''''\n",
    "            #print(\"outf\", out_f)\n",
    "            \n",
    "            # setting the parameters for the entire net to be zero\n",
    "            for net in nets:\n",
    "                net.zero_grad()\n",
    "\n",
    "            \n",
    "            \n",
    "            targett = Target[molecule]\n",
    "            targett = torch.from_numpy(np.array(targett))\n",
    "            targett = targett.float()\n",
    "            targett = targett.view(1,-1)\n",
    "            ''''''\n",
    "            # get list of used atoms i.e NNP's\n",
    "            used_atoms = []\n",
    "            for atom in range(23):\n",
    "                if(Atomic_Num[molecule][atom]!=0):\n",
    "                    used_atoms.append(Atomic_Num[molecule][atom])\n",
    "            used_atoms = np.unique(np.array(used_atoms))\n",
    "            \n",
    "            # if no nets were used, just move on to the next molecule\n",
    "            if(len(used_atoms)==0) :\n",
    "                print(\"scream\")\n",
    "                break\n",
    "            \n",
    "            ''''''\n",
    "            # use loss function\n",
    "            loss = criterion(out_f,targett)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # step only the used optimizers            \n",
    "            for atom in used_atoms:\n",
    "                optimizers[atom-1].step()    \n",
    "            ''''''\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    losses = []\n",
    "    for molecule in range(301,500) :\n",
    "\n",
    "        molecule_out = torch.zeros(1,1)\n",
    "        for atom in range(23):\n",
    "            #print(Atomic_Num[molecule][atom], end=\" \")\n",
    "            if(Atomic_Num[molecule][atom]==0):\n",
    "                continue\n",
    "            aev = torch.from_numpy((AEVs[molecule][atom]))\n",
    "            net_out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "            #print(net_out)\n",
    "            molecule_out = molecule_out + net_out\n",
    "        #print(\"\")\n",
    "\n",
    "        targett = Target[molecule]\n",
    "        targett = torch.from_numpy(np.array(targett))\n",
    "        targett = targett.float()\n",
    "        targett = targett.view(1, -1)\n",
    "        #losses.append(targett - molecule_out)\n",
    "        loss = criterion(molecule_out, targett)\n",
    "        if(molecule%10==0):\n",
    "            print(\"molecule\")\n",
    "            print(\"Target: \", targett)\n",
    "            print(\"Our_vl: \", molecule_out)\n",
    "            print(\"Loss  : \", loss)\n",
    "            #print(targett - molecule_out)\n",
    "    '''\n",
    "    train(nets, optimizers, criterion)\n",
    "    testing(nets)\n",
    "    '''\n",
    "            \n",
    "    \n",
    "doNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for molecule in range(10):\n",
    "    for atom in range(23):\n",
    "        aev = []\n",
    "        for elem in AEVs[molecule][atom]:\n",
    "            if(elem>0.0000000001):\n",
    "                aev.append(elem)\n",
    "        print(aev)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
