{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(7165, 23, 520)\n",
      "(7165, 23)\n",
      "(7165,)\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Get input feature dataset\n",
    "features = scipy.io.loadmat('./feature_vector.mat')\n",
    "\n",
    "\n",
    "\n",
    "# Turn feature dataset into seperate arrays\n",
    "AEVs = np.transpose(np.array(features['AEVs']), (2, 0, 1))\n",
    "Atomic_Num = np.array(features['Atomic_Num'], dtype=np.long)\n",
    "Target = np.array(features['labels'][0])\n",
    "'''\n",
    "\n",
    "AEVs = np.random.rand(7165, 23, 520)\n",
    "Atomic_Num = np.random.randint(6, size=(7165, 23))\n",
    "Target = np.random.rand(7165)\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(np.shape(AEVs))\n",
    "print(np.shape(Atomic_Num))\n",
    "print(np.shape(Target))\n",
    "\n",
    "list_atoms = []\n",
    "for row in Atomic_Num:\n",
    "    for elem in row:\n",
    "        list_atoms.append(elem)\n",
    "print(np.unique(np.array(list_atoms)))\n",
    "\n",
    "# Seperate into training and testing samples\n",
    "#train_atoms =    \n",
    "#train_samples = \n",
    "#train_labels = \n",
    "\n",
    "#test_atoms = \n",
    "#test_samples = \n",
    "#test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(520, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = (self.fc4(x))\n",
    "        #print(x)\n",
    "        return x #need activation function on x or loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "molecule\n",
      "Target:  tensor([[-1131.6400]])\n",
      "Our_vl:  tensor([[3.8696e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.4974e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1317.0800]])\n",
      "Our_vl:  tensor([[4.3319e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.8765e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1343.7600]])\n",
      "Our_vl:  tensor([[4.3319e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.8765e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1124.1500]])\n",
      "Our_vl:  tensor([[4.3533e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.8952e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1353.0200]])\n",
      "Our_vl:  tensor([[1.7096e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.9226e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1750.7400]])\n",
      "Our_vl:  tensor([[2.9885e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(8.9311e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1620.3101]])\n",
      "Our_vl:  tensor([[1.6881e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.8496e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1608.8000]])\n",
      "Our_vl:  tensor([[1.6881e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.8496e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1253.8800]])\n",
      "Our_vl:  tensor([[3.0314e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(9.1897e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1494.9200]])\n",
      "Our_vl:  tensor([[3.0100e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(9.0599e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1314.5100]])\n",
      "Our_vl:  tensor([[4.3319e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.8765e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1471.8800]])\n",
      "Our_vl:  tensor([[3.0100e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(9.0599e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1224.2200]])\n",
      "Our_vl:  tensor([[5.6537e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3.1965e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1365.7700]])\n",
      "Our_vl:  tensor([[4.3319e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.8765e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1767.1100]])\n",
      "Our_vl:  tensor([[3.6620e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.3410e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1480.5800]])\n",
      "Our_vl:  tensor([[5.6323e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3.1723e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1760.2500]])\n",
      "Our_vl:  tensor([[3.6620e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.3410e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1191.4301]])\n",
      "Our_vl:  tensor([[3.0314e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(9.1897e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1762.0300]])\n",
      "Our_vl:  tensor([[3.6620e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.3410e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1754.9000]])\n",
      "Our_vl:  tensor([[3.6620e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.3410e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1636.4600]])\n",
      "Our_vl:  tensor([[-9.3421e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(8.7276e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1509.6899]])\n",
      "Our_vl:  tensor([[3.8767e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.5029e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1466.3101]])\n",
      "Our_vl:  tensor([[3.0100e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(9.0599e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1640.4000]])\n",
      "Our_vl:  tensor([[-9.3421e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(8.7276e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1472.1500]])\n",
      "Our_vl:  tensor([[3.8767e+14]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1.5029e+29, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1485.7200]])\n",
      "Our_vl:  tensor([[5.6323e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3.1723e+31, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1609.7300]])\n",
      "Our_vl:  tensor([[1.6881e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.8496e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1616.1899]])\n",
      "Our_vl:  tensor([[1.6881e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.8496e+30, grad_fn=<MseLossBackward>)\n",
      "molecule\n",
      "Target:  tensor([[-1362.8500]])\n",
      "Our_vl:  tensor([[1.7096e+15]], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2.9226e+30, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def doNN():\n",
    "    # Declare Nets\n",
    "    NNP_H = Net()\n",
    "    NNP_C = Net()\n",
    "    NNP_N = Net()\n",
    "    NNP_O = Net()\n",
    "    NNP_S = Net()\n",
    "    \n",
    "    # put in a list for ease of access\n",
    "    nets = [NNP_H, NNP_C, NNP_N, NNP_O, NNP_S]\n",
    "\n",
    "        \n",
    "    # corresponding optimizers\n",
    "    optimizers = []\n",
    "    #criterions = []\n",
    "    for net in nets:\n",
    "        optimizers.append( optim.SGD(net.parameters(), lr=0.0001, momentum = 0.99) )\n",
    "        #criterions.append(nn.MSELoss())\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # wonder if we need seperate criterions\n",
    "    #criterion = nn.MSELoss()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    # training\n",
    "    epochs = 5\n",
    "    molecules = 200\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        print(\"epoch: \", epoch)\n",
    "        outp = []\n",
    "            \n",
    "        for molecule in range(molecules):\n",
    "        \n",
    "            # initialize optimizer\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "            out_f = torch.zeros(1,1)\n",
    "            \n",
    "            # use all relevant aev's on the relevant nets\n",
    "            for atom in range(23):\n",
    "                #print(Atomic_Num[molecule][atom], end=\" \"),\n",
    "                if(Atomic_Num[molecule][atom]==0):\n",
    "                    continue\n",
    "                \n",
    "                aev = torch.from_numpy(AEVs[molecule][atom])\n",
    "                out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "                #print(\"out\", out)\n",
    "                out_f = out_f + out\n",
    "                \n",
    "                '''\n",
    "                #extra\n",
    "                targett = Target[molecule]\n",
    "                targett = torch.from_numpy(np.array(targett)\n",
    "                targett = targett.view(1,-1)\n",
    "                loss = criterion(out, targett.float())\n",
    "                loss.backward()\n",
    "                optimizers[Atomic_Num[molecule][atom]-1].step()\n",
    "                '''\n",
    "            \n",
    "        \n",
    "            ''''''\n",
    "            #print(\"outf\", out_f)\n",
    "            \n",
    "            # setting the parameters for the entire net to be zero\n",
    "            for net in nets:\n",
    "                net.zero_grad()\n",
    "\n",
    "            \n",
    "            \n",
    "            targett = Target[molecule]\n",
    "            targett = torch.from_numpy(np.array(targett))\n",
    "            targett = targett.float()\n",
    "            targett = targett.view(1,-1)\n",
    "            ''''''\n",
    "            # get list of used atoms i.e NNP's\n",
    "            used_atoms = []\n",
    "            for atom in range(23):\n",
    "                if(Atomic_Num[molecule][atom]!=0):\n",
    "                    used_atoms.append(Atomic_Num[molecule][atom])\n",
    "            used_atoms = np.unique(np.array(used_atoms))\n",
    "            \n",
    "            # if no nets were used, just move on to the next molecule\n",
    "            if(len(used_atoms)==0) :\n",
    "                print(\"scream\")\n",
    "                break\n",
    "            \n",
    "            ''''''\n",
    "            # use loss function\n",
    "            loss = criterion(out_f,targett)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # step only the used optimizers            \n",
    "            for atom in used_atoms:\n",
    "                optimizers[atom-1].step()    \n",
    "            ''''''\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    losses = []\n",
    "    for molecule in range(201,500) :\n",
    "\n",
    "        molecule_out = torch.zeros(1,1)\n",
    "        for atom in range(23):\n",
    "            #print(Atomic_Num[molecule][atom], end=\" \")\n",
    "            if(Atomic_Num[molecule][atom]==0):\n",
    "                continue\n",
    "            aev = torch.from_numpy(AEVs[molecule][atom])\n",
    "            net_out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "            #print(net_out)\n",
    "            molecule_out = molecule_out + net_out\n",
    "        #print(\"\")\n",
    "\n",
    "        targett = Target[molecule]\n",
    "        targett = torch.from_numpy(np.array(targett))\n",
    "        targett = targett.float()\n",
    "        targett = targett.view(1, -1)\n",
    "        #losses.append(targett - molecule_out)\n",
    "        loss = criterion(molecule_out, targett)\n",
    "        if(molecule%10==0):\n",
    "            print(\"molecule\")\n",
    "            print(\"Target: \", targett)\n",
    "            print(\"Our_vl: \", molecule_out)\n",
    "            print(\"Loss  : \", loss)\n",
    "            #print(targett - molecule_out)\n",
    "    '''\n",
    "    train(nets, optimizers, criterion)\n",
    "    testing(nets)\n",
    "    '''\n",
    "            \n",
    "    \n",
    "doNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for molecule in range(10):\n",
    "    for atom in range(23):\n",
    "        aev = []\n",
    "        for elem in AEVs[molecule][atom]:\n",
    "            if(elem>0.0000000001):\n",
    "                aev.append(elem)\n",
    "        print(aev)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
