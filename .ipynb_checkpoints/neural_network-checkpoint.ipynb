{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(520, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x #need activation function on x or loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(7165, 23, 520)\n",
      "(7165, 23)\n",
      "(7165,)\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Get input feature dataset\n",
    "features = scipy.io.loadmat('./feature_vector 2.mat')\n",
    "\n",
    "\n",
    "# Turn feature dataset into seperate arrays\n",
    "AEVs = np.transpose(np.array(features['AEVs']), (2, 0, 1))\n",
    "Atomic_Num = np.array(features['Atomic_Num'], dtype=np.long)\n",
    "Target = np.array(features['labels'][0])\n",
    "'''\n",
    "\n",
    "AEVs = np.random.rand(7165, 23, 520)\n",
    "Atomic_Num = np.random.randint(6, size=(7165, 23))\n",
    "Target = np.random.rand(7165)\n",
    "'''\n",
    "print(\"Shapes:\")\n",
    "print(np.shape(AEVs))\n",
    "print(np.shape(Atomic_Num))\n",
    "print(np.shape(Target))\n",
    "\n",
    "list_atoms = []\n",
    "for row in Atomic_Num:\n",
    "    for elem in row:\n",
    "        list_atoms.append(elem)\n",
    "print(np.unique(np.array(list_atoms)))\n",
    "\n",
    "# Seperate into training and testing samples\n",
    "#train_atoms =    \n",
    "#train_samples = \n",
    "#train_labels = \n",
    "\n",
    "#test_atoms = \n",
    "#test_samples = \n",
    "#test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1249.4500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1561125.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1249.4500]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1100.5900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1211298.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1100.5900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-945.6100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(894178.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-945.6100]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1400.2900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1960812.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1400.2900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1600.2700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2560864.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1600.2700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1123.3199]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1261847.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1123.3199]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1258.4600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1583721.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1258.4600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1132.0100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1281446.6250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1132.0100]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1258.2800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1583268.6250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1258.2800]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1131.6400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1280609.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1131.6400]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1401.1200]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1963137.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1401.1200]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1406.4700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1978157.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1406.4700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1270.0601]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1613052.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1270.0601]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1104.1899]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1219235.3750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1104.1899]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-972.4900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(945736.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-972.4900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1194.1600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1426018.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1194.1600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1602.2800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2567301.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1602.2800]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1751.2000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3066701.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1751.2000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1595.9500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2547056.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1595.9500]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 2 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1317.0800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1734699.6250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1317.0800]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 3 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1230.1400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1513244.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1230.1400]], grad_fn=<SubBackward0>)\n",
      "2 2 3 3 2 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1213.2400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1471951.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1213.2400]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1468.]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2155024., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1468.]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1462.7400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2139608.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1462.7400]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1482.8000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2198696., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1482.8000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1485.7600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2207482.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1485.7600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1750.7000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3064950.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1750.7000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1484.8700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2204839., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1484.8700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1488.4700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2215542.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1488.4700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1343.7600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1805691., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1343.7600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1348.1000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1817373.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1348.1000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1496.5200]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2239572.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1496.5200]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1257.1300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1580375.8750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1257.1300]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1639.5800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2688222.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1639.5800]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1242.6700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1544228.8750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1242.6700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 3 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1105.6700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1222506.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1105.6700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 3 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1217.2300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1481648.8750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1217.2300]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1750.5300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3064355.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1750.5300]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1472.]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2166784., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1472.]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 2 3 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1124.1500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1263713.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1124.1500]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 2 3 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1503.9800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2261955.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1503.9800]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 3 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1129.2600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1275228.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1129.2600]], grad_fn=<SubBackward0>)\n",
      "2 2 3 2 3 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1383.1100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1912993.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1383.1100]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 3 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1206.6801]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1456076.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1206.6801]], grad_fn=<SubBackward0>)\n",
      "2 2 3 3 2 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1363.9900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1860468.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1363.9900]], grad_fn=<SubBackward0>)\n",
      "2 3 2 2 2 3 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1335.7300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1784174.6250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1335.7300]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1481.1700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2193864.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1481.1700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1484.5000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2203740.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1484.5000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1748.5601]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3057462.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1748.5601]], grad_fn=<SubBackward0>)\n",
      "2 3 2 2 3 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1353.0200]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1830663.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1353.0200]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1220.2300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1488961.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1220.2300]], grad_fn=<SubBackward0>)\n",
      "2 3 3 2 2 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1220.9900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1490816.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1220.9900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1621.3500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2628775.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1621.3500]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1621.3600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2628808.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1621.3600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1361.0200]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1852375.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1361.0200]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1615.5500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2610002., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1615.5500]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1622.2100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2631565.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1622.2100]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1619.7000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2623428., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1619.7000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1637.3800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2681013.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1637.3800]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1750.7400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3065090.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1750.7400]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1621.1801]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2628224.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1621.1801]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1621.3199]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2628678.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1621.3199]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 3 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1387.3700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1924795.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1387.3700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1612.5601]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2600350., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1612.5601]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1613.9500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2604834.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1613.9500]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1635.9399]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2676299.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1635.9399]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1492.6000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2227854.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1492.6000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1497.0699]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2241218.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1497.0699]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1614.9700]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2608128., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1614.9700]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1620.3101]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2625404.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1620.3101]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1749.3600]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3060260.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1749.3600]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1386.1100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1921300.8750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1386.1100]], grad_fn=<SubBackward0>)\n",
      "2 2 3 3 2 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1255.9399]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1577385.1250, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1255.9399]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 3 3 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1363.6899]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1859650.2500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1363.6899]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1347.8900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1816807.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1347.8900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 3 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1324.3300]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1753849.8750, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1324.3300]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1614.8900]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2607869.7500, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1614.8900]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1489.5800]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2218848.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1489.5800]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1620.5400]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2626150., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1620.5400]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1608.8000]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2588237.5000, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1608.8000]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1242.7100]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(1544328., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1242.7100]], grad_fn=<SubBackward0>)\n",
      "2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1750.3500]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(3063725., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1750.3500]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2 3 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 \n",
      "molecule\n",
      "Target:  tensor([[-1487.9200]])\n",
      "Our_vl:  tensor([0.], grad_fn=<AddBackward0>)\n",
      "Loss  :  tensor(2213906., grad_fn=<MseLossBackward>)\n",
      "tensor([[-1487.9200]], grad_fn=<SubBackward0>)\n",
      "2 2 2 3 2"
     ]
    }
   ],
   "source": [
    "def doNN():\n",
    "    # Declare Nets\n",
    "    NNP_H = Net()\n",
    "    NNP_C = Net()\n",
    "    NNP_N = Net()\n",
    "    NNP_O = Net()\n",
    "    NNP_S = Net()\n",
    "    \n",
    "    # put in a list for ease of access\n",
    "    nets = [NNP_H, NNP_C, NNP_N, NNP_O, NNP_S]\n",
    "\n",
    "    # setting the parameters for the entire net to be zero\n",
    "    for net in nets:\n",
    "        net.zero_grad()\n",
    "    \n",
    "    # corresponding optimizers\n",
    "    optimizers = []\n",
    "    #criterions = []\n",
    "    for net in nets:\n",
    "        optimizers.append( optim.SGD(net.parameters(), lr=0.01, momentum = 0.9) )\n",
    "        #criterions.append(nn.MSELoss())\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # wonder if we need seperate criterions\n",
    "    #criterion = nn.MSELoss()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    # training\n",
    "    epochs = 3\n",
    "    molecules = 200\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        print(\"epoch: \", epoch)\n",
    "        outp = []\n",
    "            \n",
    "        for molecule in range(molecules):\n",
    "        \n",
    "            # initialize optimizer\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "            out_f = 0;\n",
    "            \n",
    "            # use all relevant aev's on the relevant nets\n",
    "            for atom in range(23):\n",
    "                #print(Atomic_Num[molecule][atom], end=\" \"),\n",
    "                if(Atomic_Num[molecule][atom]==0):\n",
    "                    continue\n",
    "                \n",
    "                aev = torch.from_numpy(AEVs[molecule][atom])\n",
    "                out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "                out_f = out_f + out\n",
    "                \n",
    "                '''\n",
    "                #extra\n",
    "                targett = Target[molecule]\n",
    "                targett = torch.from_numpy(np.array(targett)\n",
    "                targett = targett.view(1,-1)\n",
    "                loss = criterion(out, targett.float())\n",
    "                loss.backward()\n",
    "                optimizers[Atomic_Num[molecule][atom]-1].step()\n",
    "                '''\n",
    "            \n",
    "            ''''''\n",
    "            targett = Target[molecule]\n",
    "            targett = torch.from_numpy(np.array(targett))\n",
    "            targett = targett.float()\n",
    "            targett = targett.view(1,-1)\n",
    "            ''''''\n",
    "            # get list of used atoms i.e NNP's\n",
    "            used_atoms = []\n",
    "            for atom in range(23):\n",
    "                if(Atomic_Num[molecule][atom]!=0):\n",
    "                    used_atoms.append(Atomic_Num[molecule][atom])\n",
    "            used_atoms = np.unique(np.array(used_atoms))\n",
    "            \n",
    "            # if no nets were used, just move on to the next molecule\n",
    "            if(len(used_atoms)==0) :\n",
    "                print(\"scream\")\n",
    "                break\n",
    "            \n",
    "            ''''''\n",
    "            # use loss function\n",
    "            loss = criterion(out_f,targett)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # step only the used optimizers            \n",
    "            for atom in used_atoms:\n",
    "                optimizers[atom-1].step()    \n",
    "            ''''''\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    losses = []\n",
    "    for molecule in range(201,500) :\n",
    "\n",
    "        molecule_out = 0\n",
    "        for atom in range(23):\n",
    "            print(Atomic_Num[molecule][atom], end=\" \")\n",
    "            if(Atomic_Num[molecule][atom]==0):\n",
    "                continue\n",
    "            aev = torch.from_numpy(AEVs[molecule][atom])\n",
    "            net_out = nets[Atomic_Num[molecule][atom]-1](aev.float())\n",
    "            molecule_out = molecule_out + net_out\n",
    "        print(\"\")\n",
    "\n",
    "        targett = Target[molecule]\n",
    "        targett = torch.from_numpy(np.array(targett))\n",
    "        targett = targett.float()\n",
    "        targett = targett.view(1, -1)\n",
    "        #losses.append(targett - molecule_out)\n",
    "        loss = criterion(molecule_out, targett)\n",
    "        print(\"molecule\")\n",
    "        print(\"Target: \", targett)\n",
    "        print(\"Our_vl: \", molecule_out)\n",
    "        print(\"Loss  : \", loss)\n",
    "        print(targett - molecule_out)\n",
    "    '''\n",
    "    train(nets, optimizers, criterion)\n",
    "    testing(nets)\n",
    "    '''\n",
    "            \n",
    "    \n",
    "doNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for molecule in range(10):\n",
    "    for atom in range(23):\n",
    "        aev = []\n",
    "        for elem in AEVs[molecule][atom]:\n",
    "            if(elem>0.0000000001):\n",
    "                aev.append(elem)\n",
    "        print(aev)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
